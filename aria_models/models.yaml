{
  "schema_version": 3,
  "notes": "SINGLE SOURCE OF TRUTH for all model definitions. To add a new model: (1) add entry here, (2) run scripts/generate_configs.py to regenerate litellm-config.yaml. That's it.",
  "model_priority_order": "local → free → paid. Use criteria.priority for the exact fallback chain. NEVER duplicate model names in .md or .py files — reference this file.",
  "validation": {
    "schema_version": 3,
    "required_fields": ["id", "name", "provider", "tier", "context_window"]
  },
  "providers": {
    "litellm": {
      "baseUrl": "http://litellm:4000/v1",
      "apiKey": "${LITELLM_MASTER_KEY}",
      "api": "openai-completions"
    }
  },
  "routing": {
    "primary": "litellm/kimi",
    "timeout": 300,
    "retries": 2,
    "fallbacks": ["litellm/qwen3-next-free", "litellm/deepseek-free", "litellm/gpt-oss-free", "litellm/trinity-free", "litellm/chimera-free"]
  },
  "agent_aliases": {
    "litellm/qwen3-mlx": "Qwen3 4B Instruct (MLX Local)",
    "litellm/trinity-free": "Trinity 400B (OpenRouter FREE)",
    "litellm/chimera-free": "Chimera 671B (OpenRouter FREE)",
    "litellm/qwen3-coder-free": "Qwen3 Coder 480B (OpenRouter FREE)",
    "litellm/qwen3-next-free": "Qwen3 Next 235B (OpenRouter FREE)",
    "litellm/glm-free": "GLM 4.5 Air (OpenRouter FREE)",
    "litellm/deepseek-free": "DeepSeek R1 (OpenRouter FREE)",
    "litellm/nemotron-free": "Nemotron 30B (OpenRouter FREE)",
    "litellm/gpt-oss-free": "GPT-OSS 120B (OpenRouter FREE)",
    "litellm/gpt-oss-small-free": "GPT-OSS 20B (OpenRouter FREE)",
    "litellm/kimi": "Kimi K2.5 (Moonshot Paid)"
  },
  "models": {
    "qwen3-mlx": {
      "provider": "litellm",
      "name": "Qwen3 4B Instruct (MLX Local)",
      "_note": "ACTIVE: 4B (2.1GB). 8B needs 24GB+ RAM with Docker. To switch: change model + plist",
      "reasoning": false,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 32768,
      "maxTokens": 8192,
      "tier": "local",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openai/mlx-community/Qwen3-4B-Instruct-2507-4bit",
        "api_base": "http://host.docker.internal:8080/v1",
        "api_key": "not-needed"
      }
    },
    "trinity-free": {
      "provider": "litellm",
      "name": "Trinity 400B MoE (OpenRouter FREE)",
      "reasoning": true,
      "tool_calling": false,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 131072,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/arcee-ai/trinity-large-preview:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY"
      }
    },
    "chimera-free": {
      "provider": "litellm",
      "name": "Chimera 671B (OpenRouter FREE)",
      "reasoning": true,
      "tool_calling": false,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 164000,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/tngtech/deepseek-r1t2-chimera:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY_DEEP"
      }
    },
    "qwen3-coder-free": {
      "provider": "litellm",
      "name": "Qwen3 Coder 480B (OpenRouter FREE)",
      "reasoning": true,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 262000,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/qwen/qwen3-coder:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY"
      }
    },
    "qwen3-next-free": {
      "provider": "litellm",
      "name": "Qwen3 Next 235B (OpenRouter FREE)",
      "reasoning": true,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 262000,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/qwen/qwen3-next-80b-a3b-instruct:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY"
      }
    },
    "glm-free": {
      "provider": "litellm",
      "name": "GLM 4.5 Air (OpenRouter FREE)",
      "reasoning": true,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 131072,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/z-ai/glm-4.5-air:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY"
      }
    },
    "deepseek-free": {
      "provider": "litellm",
      "name": "DeepSeek R1 (OpenRouter FREE)",
      "reasoning": true,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 164000,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/deepseek/deepseek-r1-0528:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY_DEEP"
      }
    },
    "nemotron-free": {
      "provider": "litellm",
      "name": "Nemotron 30B (OpenRouter FREE)",
      "reasoning": true,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 256000,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/nvidia/nemotron-3-nano-30b-a3b:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY_DEEP"
      }
    },
    "gpt-oss-free": {
      "provider": "litellm",
      "name": "GPT-OSS 120B (OpenRouter FREE)",
      "reasoning": true,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 131000,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/openai/gpt-oss-120b:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY"
      }
    },
    "gpt-oss-small-free": {
      "provider": "litellm",
      "name": "GPT-OSS 20B (OpenRouter FREE)",
      "reasoning": false,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 131000,
      "maxTokens": 8192,
      "tier": "free",
      "routeSkill": "litellm",
      "litellm": {
        "model": "openrouter/openai/gpt-oss-20b:free",
        "api_key": "os.environ/OPEN_ROUTER_KEY"
      }
    },
    "kimi": {
      "provider": "litellm",
      "name": "Kimi K2.5 (Moonshot Paid)",
      "reasoning": true,
      "input": ["text"],
      "cost": { "input": 0.56, "output": 2.94, "cacheRead": 0.098, "cacheWrite": 0 },
      "contextWindow": 256000,
      "maxTokens": 16384,
      "tier": "paid",
      "routeSkill": "litellm",
      "litellm": {
        "model": "moonshot/kimi-k2.5",
        "api_key": "os.environ/MOONSHOT_KIMI_KEY",
        "api_base": "https://api.moonshot.ai/v1"
      },
      "aliases": ["kimi-k2.5", "kimi-local"]
    },
    "kimi-k2-thinking": {
      "provider": "litellm",
      "name": "Kimi K2 Thinking (Moonshot Paid)",
      "reasoning": true,
      "input": ["text"],
      "cost": { "input": 0.56, "output": 2.24, "cacheRead": 0.14, "cacheWrite": 0 },
      "contextWindow": 256000,
      "maxTokens": 16384,
      "tier": "paid",
      "routeSkill": "litellm",
      "litellm": {
        "model": "moonshot/kimi-k2-thinking",
        "api_key": "os.environ/MOONSHOT_KIMI_KEY",
        "api_base": "https://api.moonshot.ai/v1"
      },
      "aliases": ["kimi-k2-thinking-turbo"]
    },
    "qwen-cpu-fallback": {
      "provider": "litellm",
      "name": "Qwen 2.5 3B (Ollama CPU Fallback)",
      "reasoning": false,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 32768,
      "maxTokens": 4096,
      "tier": "local",
      "routeSkill": "litellm",
      "litellm": {
        "model": "ollama/qwen2.5:3b",
        "api_base": "http://host.docker.internal:11434"
      }
    },
    "phi4-mini-local": {
      "provider": "litellm",
      "name": "Phi-4 Mini 3.8B (Ollama Local)",
      "reasoning": false,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 8192,
      "maxTokens": 4096,
      "tier": "local",
      "routeSkill": "litellm",
      "use_for": ["routing", "classification", "simple_qa"],
      "litellm": {
        "model": "ollama/phi4-mini:3.8b",
        "api_base": "http://host.docker.internal:11434"
      }
    },
    "qwen3-local": {
      "provider": "litellm",
      "name": "Qwen3 8B (Ollama Local)",
      "reasoning": false,
      "input": ["text"],
      "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
      "contextWindow": 32768,
      "maxTokens": 4096,
      "tier": "local",
      "routeSkill": "litellm",
      "use_for": ["general", "code", "analysis"],
      "litellm": {
        "model": "ollama/qwen3:8b",
        "api_base": "http://host.docker.internal:11434"
      }
    }
  },
  "criteria": {
    "priority": ["kimi", "qwen3-next-free", "deepseek-free", "trinity-free", "chimera-free"],
    "tiers": {
      "local": ["qwen3-mlx", "qwen-cpu-fallback", "phi4-mini-local", "qwen3-local"],
      "free": ["trinity-free", "qwen3-coder-free", "chimera-free", "qwen3-next-free", "glm-free", "deepseek-free", "nemotron-free", "gpt-oss-free", "gpt-oss-small-free"],
      "paid": ["kimi", "kimi-k2-thinking"]
    },
    "use_cases": {
      "code_generation": ["qwen3-coder-free", "gpt-oss-free"],
      "complex_reasoning": ["chimera-free", "deepseek-free", "kimi-k2-thinking"],
      "creative_writing": ["trinity-free"],
      "long_context": ["qwen3-next-free", "nemotron-free", "kimi"],
      "fast_simple": ["gpt-oss-small-free", "kimi"],
      "default": ["kimi"]
    },
    "focus_defaults": {
      "orchestrator": "kimi",
      "devsecops": "qwen3-coder-free",
      "data": "chimera-free",
      "trader": "deepseek-free",
      "creative": "trinity-free",
      "social": "trinity-free",
      "journalist": "qwen3-next-free"
    }
  },
  "profiles": {
    "routing":  { "model": "kimi",             "temperature": 0.3, "max_tokens": 512 },
    "analysis": { "model": "kimi",             "temperature": 0.7, "max_tokens": 4096 },
    "creative": { "model": "trinity-free",     "temperature": 0.9, "max_tokens": 2048 },
    "code":     { "model": "qwen3-coder-free", "temperature": 0.2, "max_tokens": 8192 },
    "social":   { "model": "trinity-free",     "temperature": 0.8, "max_tokens": 1024 }
  }
}
