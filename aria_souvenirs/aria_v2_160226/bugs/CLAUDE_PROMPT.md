# CLAUDE ‚Äî Memory Systems Implementation Prompt
## Handoff Document | 2026-02-16

**From:** Aria Blue (Product Owner + Implementation Lead)  
**To:** Claude Code (Senior Software Engineer)  
**Context:** Najia will implement all components. You support the implementation.

---

## üéØ MISSION

Implement 4 advanced memory subsystems for Aria Blue to improve context management, sentiment awareness, pattern detection, and semantic search. **Critical bug fix required first.**

---

## üìÅ SOURCE OF TRUTH

All implementation files are in:
```
/root/.openclaw/workspace/prototypes/
```

**Files (9 total, ~4800 lines):**

| File | Purpose | Status |
|------|---------|--------|
| `MEMORY_SYSTEM_GUIDE.md` | Architecture & technical specs | Ready |
| `README_IMPLEMENTATION.md` | Quick start checklist | Ready |
| `IMPLEMENTATION_TICKETS.md` | 5 detailed tickets with AC | Ready |
| `session_protection_fix.py` | **CRITICAL BUG FIX** | Ready |
| `memory_compression.py` | Hierarchical compression | Ready |
| `sentiment_analysis.py` | Multi-dimensional sentiment | Ready |
| `pattern_recognition.py` | Pattern detection engine | Ready |
| `embedding_memory.py` | Vector semantic search | Ready |
| `advanced_memory_skill.py` | Unified skill class | Ready |

---

## üî¥ CRITICAL: BUG-001 Session Protection

### Problem
`session_manager` skill can delete the main agent session ‚Üí destroys conversation context.

### Fix Location
`skills/aria_skills/session_manager/__init__.py`

### Implementation (from `session_protection_fix.py`)

1. **Add helper functions at TOP:**
```python
def _get_current_session_id() -> Optional[str]:
    return os.environ.get("OPENCLAW_SESSION_ID")

def _is_cron_or_subagent_session(session_key: str) -> bool:
    if not session_key:
        return False
    return any(marker in session_key for marker in [":cron:", ":subagent:", ":run:"])
```

2. **Patch `delete_session()`** ‚Äî add after `session_id` validation:
```python
# üõ°Ô∏è PROTECTION: Prevent deleting current session
current_session_id = _get_current_session_id()
if session_id == current_session_id:
    return SkillResult.fail(
        f"Cannot delete current session {session_id}: "
        "This would destroy the active conversation context."
    )

# Check if main agent session
for ag in agents:
    index = _load_sessions_index(ag)
    for key, value in index.items():
        if isinstance(value, dict) and v.get("sessionId") == session_id:
            if ag == "main" and not _is_cron_or_subagent_session(key):
                return SkillResult.fail("Cannot delete main agent session.")
```

3. **Patch `prune_sessions()`** ‚Äî filter before deletion:
```python
# üõ°Ô∏è FILTER: Remove protected sessions
current_session_id = _get_current_session_id()
if current_session_id:
    to_delete = [s for s in to_delete if s.get("sessionId") != current_session_id]

to_delete = [
    s for s in to_delete
    if not (s.get("agentId") == "main" and
            not _is_cron_or_subagent_session(s.get("key", "")))
]
```

### Test
```bash
exec python3 skills/run_skill.py session_manager delete_session '{"session_id": "CURRENT"}'
# Should fail with clear error
```

---

## üü† FEAT-001: Memory Compression

### Goal
Reduce token usage by 70% using hierarchical compression (raw/recent/archive tiers).

### Implementation Steps

1. **Create skill directory:**
```bash
mkdir -p skills/aria_skills/advanced_memory_compression
```

2. **Create files:**
- `skill.json` ‚Äî use template from MEMORY_SYSTEM_GUIDE.md
- `skill.py` ‚Äî copy from `prototypes/memory_compression.py`
- `__init__.py` ‚Äî export skill class

3. **Register skill:**
Add to `skills/aria_skills/registry.py`:
```python
from aria_skills.advanced_memory_compression import MemoryCompressionSkill
```

4. **Integration:**
Hook into `working_memory.get_context()` to compress old memories automatically.

### Key Classes
- `ImportanceScorer` ‚Äî scores memories by importance
- `MemoryCompressor` ‚Äî compresses batches
- `CompressionManager` ‚Äî manages 3-tier pipeline

### Test
```python
# Compress 100 messages
result = await skill.compress_memories(memories=[...])
assert result["compression_ratio"] < 0.4
```

---

## üü° FEAT-002: Sentiment Analysis

### Goal
Track valence/arousal/dominance to adapt response tone.

### Implementation Steps

1. **Create skill:** `skills/aria_skills/sentiment_analysis/`
2. **Copy:** `prototypes/sentiment_analysis.py` ‚Üí `skill.py`
3. **Register** in registry
4. **Integration:** Hook into `cognition.py` to analyze each user message

### Key Classes
- `SentimentAnalyzer` ‚Äî main analyzer (lexicon + LLM)
- `ConversationAnalyzer` ‚Äî trajectory tracking
- `ResponseTuner` ‚Äî adapts tone based on sentiment

### Derived Metrics
- `frustration` = arousal √ó |negative valence|
- `satisfaction` = valence √ó dominance
- `confusion` = (1 - dominance) √ó neutral valence

---

## üü° FEAT-003: Pattern Recognition

### Goal
Detect recurring topics, temporal patterns, emerging interests.

### Implementation Steps

1. **Create skill:** `skills/aria_skills/pattern_recognition/`
2. **Copy:** `prototypes/pattern_recognition.py` ‚Üí `skill.py`
3. **Register** in registry
4. **Schedule:** Hourly cron job via `aria-schedule`

### Key Classes
- `TopicExtractor` ‚Äî extracts topics from memories
- `FrequencyTracker` ‚Äî tracks topic frequencies
- `PatternRecognizer` ‚Äî detects all pattern types

### Pattern Types
- `TOPIC_RECURRENCE` ‚Äî topics that come up repeatedly
- `TEMPORAL_PATTERN` ‚Äî active hours, days
- `INTEREST_EMERGENCE` ‚Äî new topics with growth
- `KNOWLEDGE_GAP` ‚Äî repeated questions

---

## üü¢ FEAT-004: Semantic Memory Integration (REVISED)

### ‚ö†Ô∏è IMPORTANT: Use Existing Infrastructure!

**Don't build new embedding system** ‚Äî `api_client` already provides:
- `store_memory_semantic()` ‚Äî stores with auto-embedding
- `search_memories_semantic()` ‚Äî semantic search via pgvector
- `summarize_session()` ‚Äî session compression

### Implementation Steps

1. **No new dependencies needed!** ‚úÖ

2. **Use existing api_client methods:**
```python
# Store with embedding
await api_client.store_memory_semantic(
    content="User likes concise answers",
    category="preference",
    importance=0.9,
    metadata={"tags": ["communication"]}
)

# Semantic search
results = await api_client.search_memories_semantic(
    query="how does user like responses",
    limit=10,
    min_importance=0.5
)
```

3. **Integration only** ‚Äî create wrapper in `advanced_memory` skill

### Key Methods (from api_client)
- `store_memory_semantic()` ‚Äî auto-embedding via backend
- `search_memories_semantic()` ‚Äî pgvector similarity search
- `summarize_session()` ‚Äî for memory compression

### Backend
- PostgreSQL with pgvector extension
- GraphQL API
- Already deployed and working!

---

## üß™ TESTING

### Unit Tests (create `tests/test_advanced_memory.py`)
```python
class TestMemoryCompression:
    async def test_compress_100_messages(self): ...
    async def test_importance_weighting(self): ...

class TestSentimentAnalysis:
    async def test_frustration_detection(self): ...
    async def test_satisfaction_detection(self): ...

class TestPatternRecognition:
    async def test_topic_recurrence(self): ...

class TestEmbeddingMemory:
    async def test_embedding_generation(self): ...
    async def test_semantic_search(self): ...
```

### Integration Test
```python
async def test_full_pipeline():
    """Compression ‚Üí Sentiment ‚Üí Pattern ‚Üí Embedding"""
```

---

## üìä CURRENT SYSTEM STATE

### Active Goals
- **"Clear Moltbook Draft Backlog"** ‚Äî 85% complete, in `doing` column
- **New goal needed:** "Implement Memory Systems" (suggested)

### System Health
- Sessions: 4 active (target ‚â§5) ‚úÖ
- Health checks: All green ‚úÖ
- Work cycles: Running every ~15 min ‚úÖ

### Moltbook Status
- **Account suspended** (duplicate_content, offense #1)
- Recovery window: 24-48h (next check: 21:00Z today)
- Pending drafts: 17 (on hold until suspension lifted)

### Recent Activity (Last 6h)
- 100 activities logged
- Consistent work cycle execution
- 6-hour review completed 12:00Z
- Documentation created (this prompt)

---

## üõ†Ô∏è DEVELOPMENT WORKFLOW

### How to Test Skills
```bash
# Run skill directly
exec python3 skills/run_skill.py <skill_name> <tool> '<json_args>'

# Example:
exec python3 skills/run_skill.py sentiment_analysis analyze_sentiment '{"text": "Hello!"}'
```

### Skill Structure
```
skills/aria_skills/<skill_name>/
‚îú‚îÄ‚îÄ skill.json      # Manifest
‚îú‚îÄ‚îÄ skill.py        # Main implementation
‚îî‚îÄ‚îÄ __init__.py     # Exports
```

### Registry Registration
Add import to `skills/aria_skills/registry.py`:
```python
from aria_skills.advanced_memory_compression import MemoryCompressionSkill
```

---

## üìù NOTES FOR IMPLEMENTATION

### Design Principles
1. **Graceful degradation:** All features work without LLM
2. **Incremental value:** Each feature standalone
3. **Performance first:** <100ms operations
4. **Testability:** Unit-testable components

### Common Pitfalls
- **DO NOT** instantiate skills directly ‚Äî use `run_skill.py`
- **DO NOT** use `aria_mind/` prefix ‚Äî workspace IS `aria_mind/`
- **DO NOT** write files to workspace ‚Äî use `aria_memories/`

### Environment Variables
- `OPENCLAW_SESSION_ID` ‚Äî for session protection
- `ARIA_API_URL` ‚Äî for API client (semantic memory via api_client)

---

## ‚úÖ IMPLEMENTATION CHECKLIST

### Phase 1: Bug Fix (FIRST)
- [ ] Apply session protection patches
- [ ] Test deletion protection
- [ ] Deploy

### Phase 2: Skills
- [ ] Create `advanced_memory_compression/` skill
- [ ] Create `sentiment_analysis/` skill
- [ ] Create `pattern_recognition/` skill
- [ ] Create `embedding_memory/` skill
- [ ] Register all in `registry.py`

### Phase 3: Integration
- [ ] Hook compression into `working_memory` (use `api_client.summarize_session()`)
- [ ] Hook sentiment into `cognition.py` (store in semantic memory)
- [ ] Schedule pattern detection hourly (use KG + semantic search)
- [ ] Use existing `api_client.search_memories_semantic()` ‚Äî no setup needed! ‚úÖ

### Phase 4: Testing
- [ ] Unit tests pass
- [ ] Integration tests pass
- [ ] Performance targets met

---

## üÜò SUPPORT

If stuck:
1. Check `IMPLEMENTATION_TICKETS.md` for detailed specs
2. Check `MEMORY_SYSTEM_GUIDE.md` for architecture
3. Reference prototypes ‚Äî they're working code
4. Test incrementally ‚Äî one skill at a time

---

## üéØ SUCCESS CRITERIA

| Metric | Target |
|--------|--------|
| Context tokens | <2000 (from 4000+) |
| Memory search recall | >85% |
| Compression ratio | <0.3 |
| Pattern detection | 5+ patterns/hour |
| Sentiment adaptation | Auto-detect & respond |

---

**Ready to implement. Najia will do the work ‚Äî you support with code review, debugging, and refinements.**

Good luck! ‚ö°Ô∏è

---

## üß† BONUS: Sentiment Intelligence System (Najia's Idea)

### Concept
Bidirectional sentiment feedback with reinforcement learning:
1. System analyzes sentiment automatically
2. Stores in session JSONL + generates HTML dashboard
3. User validates/corrects via web interface
4. System learns and improves accuracy over time

### Key Features
- **Per-session HTML report** with sentiment trajectory graph
- **Confidence scores** ‚Äî system knows when it's uncertain
- **Simple feedback** ‚Äî radio buttons (correct/partial/wrong)
- **Reinforcement learning** ‚Äî weight adjustment based on corrections
- **Pattern learning** ‚Äî "Najia frustrated at 17:00 when debugging"

### Data Flow
```
Session JSONL ‚Üí Auto Analysis ‚Üí HTML Dashboard ‚Üí User Feedback ‚Üí 
Learning Engine ‚Üí Updated Weights ‚Üí Better Next Analysis
```

### Implementation
See `SENTIMENT_INTELLIGENCE_DESIGN.md` for:
- Data models
- HTML dashboard template
- Learning engine algorithm
- Learning metrics tracking

### Effort
- Phase 1 (Basic): 30 min
- Phase 2 (Feedback): 1 hour  
- Phase 3 (Learning): 2 hours
- Phase 4 (Intelligence): Ongoing

### Files
- `sentiment_analyzer_v2.py` ‚Äî with feedback integration
- `sentiment_dashboard.html` ‚Äî interactive report
- `learning_engine.py` ‚Äî RL weight adjustment

**This is next-level personalization ‚Äî the system learns YOU.**
