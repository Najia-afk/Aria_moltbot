{
  "name": "aria-litellm",
  "version": "1.0.0",
  "description": "Manage LiteLLM proxy, models, and API spend tracking.",
  "author": "Aria Team",
  "layer": 2,
  "dependencies": [
    "api_client"
  ],
  "focus_affinity": [
    "orchestrator"
  ],
  "tools": [
    {
      "name": "list_models",
      "description": "List available models from LiteLLM proxy.",
      "parameters": {
        "type": "object",
        "properties": {}
      }
    },
    {
      "name": "health_check",
      "description": "Check LiteLLM proxy health status.",
      "parameters": {
        "type": "object",
        "properties": {}
      }
    },
    {
      "name": "get_metrics",
      "description": "Get API spend logs, usage data, and metrics.",
      "parameters": {
        "type": "object",
        "properties": {
          "limit": {
            "type": "integer",
            "description": "Maximum log entries (default: 50)"
          },
          "model": {
            "type": "string",
            "description": "Filter by model name"
          }
        }
      }
    },
    {
      "name": "chat_completion",
      "description": "Send a chat completion request through LiteLLM proxy.",
      "parameters": {
        "type": "object",
        "properties": {
          "messages": {
            "type": "array",
            "description": "Array of chat messages with role and content"
          },
          "model": {
            "type": "string",
            "description": "Model to use"
          }
        },
        "required": [
          "messages"
        ]
      }
    }
  ],
  "run": "python3 /app/skills/run_skill.py litellm {{tool}} '{{args_json}}'"
}
