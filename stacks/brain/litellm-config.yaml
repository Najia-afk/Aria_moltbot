model_list:
  # Local Ollama model (native Metal GPU on host)
  # Primary model - GLM-4.7-Flash-REAP - smart text reasoning (per SOUL.md)
  - model_name: local-default
    litellm_params:
      model: ollama/hf.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF:Q3_K_S
      api_base: http://host.docker.internal:11434
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # GLM-4.7 - Primary brain for Aria (smart text reasoning)
  - model_name: glm-local
    litellm_params:
      model: ollama/hf.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF:Q3_K_S
      api_base: http://host.docker.internal:11434
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Qwen3-VL - Vision model for image tasks
  - model_name: qwen3-vl
    litellm_params:
      model: ollama/qwen3-vl:8b
      api_base: http://host.docker.internal:11434
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Alias for backwards compatibility - routes qwen-local to GLM
  - model_name: qwen-local
    litellm_params:
      model: ollama/hf.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF:Q3_K_S
      api_base: http://host.docker.internal:11434
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Alias for OpenClaw - routes gpt-4o requests to local GLM
  - model_name: gpt-4o
    litellm_params:
      model: ollama/hf.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF:Q3_K_S
      api_base: http://host.docker.internal:11434
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Moonshot / Kimi (256K context)
  - model_name: kimi-k2.5
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # Alias for OpenClaw fallback
  - model_name: kimi-local
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: kimi-k2-0905-preview
    litellm_params:
      model: moonshot/kimi-k2-0905-preview
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-turbo-preview
    litellm_params:
      model: moonshot/kimi-k2-turbo-preview
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-thinking
    litellm_params:
      model: moonshot/kimi-k2-thinking
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-thinking-turbo
    litellm_params:
      model: moonshot/kimi-k2-thinking-turbo
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

router_settings:
  routing_strategy: simple-shuffle
  num_retries: 3
  timeout: 120
  allowed_fails: 3
  cooldown_time: 60

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/LITELLM_DATABASE_URL
  
litellm_settings:
  drop_params: true
  set_verbose: false
