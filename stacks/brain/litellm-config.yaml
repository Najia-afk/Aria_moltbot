model_list:
  # Qwen3-VLTO 8B via MLX (Primary - Native Apple Silicon)
  - model_name: qwen3-mlx
    litellm_params:
      model: openai/nightmedia/Qwen3-VLTO-8B-Instruct-qx86x-hi-mlx
      api_base: http://host.docker.internal:8080/v1
      api_key: not-needed
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # ============================================
  # OpenRouter FREE Models
  # ============================================
  
  # GLM 4.5 Air - FREE, agent-focused with tools support
  - model_name: glm-free
    litellm_params:
      model: openrouter/z-ai/glm-4.5-air:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0
      output_cost_per_token: 0

  # DeepSeek R1 - FREE, reasoning model (like o1)
  - model_name: deepseek-free
    litellm_params:
      model: openrouter/deepseek/deepseek-r1-0528:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 164000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # NVIDIA Nemotron - FREE, 256K context, agentic
  - model_name: nemotron-free
    litellm_params:
      model: openrouter/nvidia/nemotron-3-nano-30b-a3b:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 256000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # OpenAI GPT-OSS-120B - FREE, tools/function calling
  - model_name: gpt-oss-free
    litellm_params:
      model: openrouter/openai/gpt-oss-120b:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0
      output_cost_per_token: 0

  # GLM-4.7 via Ollama (disabled - use MLX instead)
  # - model_name: glm-local
  #   litellm_params:
  #     model: ollama_chat/hf.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF:Q3_K_S
  #     api_base: http://host.docker.internal:11434
  #     stop: ["<|user|>", "<|observation|>", "[GORILLA]", "[system_message]"]
  #   model_info:
  #     max_tokens: 32768
  #     input_cost_per_token: 0
  #     output_cost_per_token: 0

  # Qwen3-VL via Ollama (3rd fallback - disabled)
  # - model_name: qwen3-vl
  #   litellm_params:
  #     model: ollama_chat/qwen3-vl:8b
  #     api_base: http://host.docker.internal:11434
  #     stop: ["<|user|>", "<|observation|>", "[GORILLA]", "[system_message]"]
  #   model_info:
  #     max_tokens: 32768
  #     input_cost_per_token: 0
  #     output_cost_per_token: 0

  # Moonshot / Kimi (256K context)
  - model_name: kimi-k2.5
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # Alias for shorter name
  - model_name: kimi
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # Alias for OpenClaw fallback
  - model_name: kimi-local
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: kimi-k2-0905-preview
    litellm_params:
      model: moonshot/kimi-k2-0905-preview
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-turbo-preview
    litellm_params:
      model: moonshot/kimi-k2-turbo-preview
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-thinking
    litellm_params:
      model: moonshot/kimi-k2-thinking
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-thinking-turbo
    litellm_params:
      model: moonshot/kimi-k2-thinking-turbo
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

router_settings:
  routing_strategy: simple-shuffle
  num_retries: 3
  timeout: 600
  allowed_fails: 3
  cooldown_time: 60

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/LITELLM_DATABASE_URL
  
litellm_settings:
  drop_params: true
  disable_streaming: true
  set_verbose: false
