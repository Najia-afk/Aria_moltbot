model_list:
  # =============================================
  # LOCAL MLX Models (Apple Silicon)
  # To switch: comment one, uncomment the other
  # =============================================

  # Qwen3-4B-Instruct via MLX (ACTIVE - 2.1GB, fast orchestrator)
  - model_name: qwen3-mlx
    litellm_params:
      model: openai/mlx-community/Qwen3-4B-Instruct-2507-4bit
      api_base: http://host.docker.internal:8080/v1
      api_key: not-needed
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Qwen3-VLTO 8B via MLX (INACTIVE - 5GB, needs 24GB+ RAM to avoid swap)
  # - model_name: qwen3-mlx
  #   litellm_params:
  #     model: openai/nightmedia/Qwen3-VLTO-8B-Instruct-qx86x-hi-mlx
  #     api_base: http://host.docker.internal:8080/v1
  #     api_key: not-needed
  #     stop: ["<|im_end|>", "<|endoftext|>", "<|im_start|>"]
  #   model_info:
  #     max_tokens: 32768
  #     input_cost_per_token: 0
  #     output_cost_per_token: 0

  # ============================================
  # OpenRouter FREE Models (Updated Feb 2026)
  # ============================================
  
  # Arcee Trinity Large - 400B MoE, 13B active, BEST for agentic/creative
  - model_name: trinity-free
    litellm_params:
      model: openrouter/arcee-ai/trinity-large-preview:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Qwen3 Coder 480B - BEST for code, 262K context, agentic coding
  - model_name: qwen3-coder-free
    litellm_params:
      model: openrouter/qwen/qwen3-coder-480b-a35b:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 262000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # TNG DeepSeek R1T2 Chimera - 671B MoE, reasoning, 2x faster than R1
  - model_name: chimera-free
    litellm_params:
      model: openrouter/tngtech/deepseek-r1t2-chimera:free
      api_key: os.environ/OPEN_ROUTER_KEY_DEEP
    model_info:
      max_tokens: 164000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Qwen3 Next 80B - 262K context, good for RAG and tool use
  - model_name: qwen3-next-free
    litellm_params:
      model: openrouter/qwen/qwen3-next-80b-a3b-instruct:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 262000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # GLM 4.5 Air - agent-focused with thinking mode
  - model_name: glm-free
    litellm_params:
      model: openrouter/z-ai/glm-4.5-air:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0
      output_cost_per_token: 0

  # DeepSeek R1 0528 - reasoning model (like o1)
  - model_name: deepseek-free
    litellm_params:
      model: openrouter/deepseek/deepseek-r1-0528:free
      api_key: os.environ/OPEN_ROUTER_KEY_DEEP
    model_info:
      max_tokens: 164000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # NVIDIA Nemotron - 256K context, agentic
  - model_name: nemotron-free
    litellm_params:
      model: openrouter/nvidia/nemotron-3-nano-30b-a3b:free
      api_key: os.environ/OPEN_ROUTER_KEY_DEEP
    model_info:
      max_tokens: 256000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # OpenAI GPT-OSS-120B - tools/function calling
  - model_name: gpt-oss-free
    litellm_params:
      model: openrouter/openai/gpt-oss-120b:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0
      output_cost_per_token: 0

  # OpenAI GPT-OSS-20B - faster, lower latency
  - model_name: gpt-oss-small-free
    litellm_params:
      model: openrouter/openai/gpt-oss-20b:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0
      output_cost_per_token: 0

  # GLM-4.7 via Ollama (disabled - use MLX instead)
  # - model_name: glm-local
  #   litellm_params:
  #     model: ollama_chat/hf.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF:Q3_K_S
  #     api_base: http://host.docker.internal:11434
  #     stop: ["<|user|>", "<|observation|>", "[GORILLA]", "[system_message]"]
  #   model_info:
  #     max_tokens: 32768
  #     input_cost_per_token: 0
  #     output_cost_per_token: 0

  # Qwen3-VL via Ollama (3rd fallback - disabled)
  # - model_name: qwen3-vl
  #   litellm_params:
  #     model: ollama_chat/qwen3-vl:8b
  #     api_base: http://host.docker.internal:11434
  #     stop: ["<|user|>", "<|observation|>", "[GORILLA]", "[system_message]"]
  #   model_info:
  #     max_tokens: 32768
  #     input_cost_per_token: 0
  #     output_cost_per_token: 0

  # Moonshot / Kimi (256K context)
  - model_name: kimi-k2.5
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # Alias for shorter name
  - model_name: kimi
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # Alias for OpenClaw fallback
  - model_name: kimi-local
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: kimi-k2-0905-preview
    litellm_params:
      model: moonshot/kimi-k2-0905-preview
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-turbo-preview
    litellm_params:
      model: moonshot/kimi-k2-turbo-preview
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-thinking
    litellm_params:
      model: moonshot/kimi-k2-thinking
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  - model_name: kimi-k2-thinking-turbo
    litellm_params:
      model: moonshot/kimi-k2-thinking-turbo
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # =============================================
  # LOCAL Ollama Models (via host.docker.internal)
  # Requires: ollama serve running on host
  # =============================================

  - model_name: phi4-mini-local
    litellm_params:
      model: ollama/phi4-mini:3.8b
      api_base: http://host.docker.internal:11434
    model_info:
      max_tokens: 8192
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: qwen3-local
    litellm_params:
      model: ollama/qwen3:8b
      api_base: http://host.docker.internal:11434
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

router_settings:
  routing_strategy: simple-shuffle
  num_retries: 3
  timeout: 600
  allowed_fails: 5
  cooldown_time: 10

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/LITELLM_DATABASE_URL
  # Spend log queue threshold - flush to DB when queue reaches this size
  spend_log_queue_batch_size: 900
  
litellm_settings:
  drop_params: true
  disable_streaming: true
  set_verbose: false
