# LiteLLM proxy config â€” cloud models only
# Ollama models excluded (require local ollama server running)
# To add ollama models: start ollama first, then regenerate with: python scripts/generate_configs.py
model_list:
# === Primary: Kimi (default model) ===
- model_name: kimi
  litellm_params:
    model: moonshot/kimi-k2.5
    api_key: os.environ/MOONSHOT_KIMI_KEY
    api_base: https://api.moonshot.ai/v1
  model_info:
    max_tokens: 16384
- model_name: kimi-k2.5
  litellm_params:
    model: moonshot/kimi-k2.5
    api_key: os.environ/MOONSHOT_KIMI_KEY
    api_base: https://api.moonshot.ai/v1
  model_info:
    max_tokens: 16384
- model_name: kimi-local
  litellm_params:
    model: moonshot/kimi-k2.5
    api_key: os.environ/MOONSHOT_KIMI_KEY
    api_base: https://api.moonshot.ai/v1
  model_info:
    max_tokens: 16384
- model_name: kimi-k2-thinking
  litellm_params:
    model: moonshot/kimi-k2-thinking
    api_key: os.environ/MOONSHOT_KIMI_KEY
    api_base: https://api.moonshot.ai/v1
  model_info:
    max_tokens: 16384
- model_name: kimi-k2-thinking-turbo
  litellm_params:
    model: moonshot/kimi-k2-thinking
    api_key: os.environ/MOONSHOT_KIMI_KEY
    api_base: https://api.moonshot.ai/v1
  model_info:
    max_tokens: 16384
# === Free fallbacks (OpenRouter) ===
- model_name: deepseek-free
  litellm_params:
    model: openrouter/deepseek/deepseek-r1-0528:free
    api_key: os.environ/OPEN_ROUTER_KEY_DEEP
  model_info:
    max_tokens: 8192
    input_cost_per_token: 0
    output_cost_per_token: 0
- model_name: qwen3-coder-free
  litellm_params:
    model: openrouter/qwen/qwen3-coder:free
    api_key: os.environ/OPEN_ROUTER_KEY
  model_info:
    max_tokens: 8192
    input_cost_per_token: 0
    output_cost_per_token: 0
- model_name: chimera-free
  litellm_params:
    model: openrouter/tngtech/deepseek-r1t2-chimera:free
    api_key: os.environ/OPEN_ROUTER_KEY_DEEP
  model_info:
    max_tokens: 8192
    input_cost_per_token: 0
    output_cost_per_token: 0
- model_name: glm-free
  litellm_params:
    model: openrouter/z-ai/glm-4.5-air:free
    api_key: os.environ/OPEN_ROUTER_KEY
  model_info:
    max_tokens: 8192
    input_cost_per_token: 0
    output_cost_per_token: 0
litellm_settings:
  drop_params: true
  disable_streaming: true
  set_verbose: false
