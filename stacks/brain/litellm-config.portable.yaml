# LiteLLM Configuration - Model Router
# 
# This configuration defines available models and routing logic.
# Priority: Local (MLX/Ollama) -> Free Cloud (OpenRouter) -> Paid Cloud (Moonshot)
#
# Environment variables required:
#   - LITELLM_MASTER_KEY: Internal authentication key
#   - MOONSHOT_KIMI_KEY: Moonshot/Kimi API key (optional)
#   - OPEN_ROUTER_KEY: OpenRouter API key (optional, free tier available)

model_list:
  # ============================================
  # LOCAL MODELS (Highest Priority - Free)
  # ============================================
  
  # MLX on Apple Silicon (Primary local - Metal GPU acceleration)
  # Only available if MLX server is running on host:8080
  - model_name: qwen3-mlx
    litellm_params:
      model: openai/nightmedia/Qwen3-VLTO-8B-Instruct-qx86x-hi-mlx
      api_base: http://host.docker.internal:8080/v1
      api_key: not-needed
      stop: ["<|im_end|>", "<|endoftext|>", "<|im_start|>"]
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Alias for local fallback (routes to best available local model)
  - model_name: local
    litellm_params:
      model: openai/nightmedia/Qwen3-VLTO-8B-Instruct-qx86x-hi-mlx
      api_base: http://host.docker.internal:8080/v1
      api_key: not-needed
      stop: ["<|im_end|>", "<|endoftext|>", "<|im_start|>"]
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0
      output_cost_per_token: 0

  # ============================================
  # OPENROUTER FREE MODELS (Secondary - Cloud Free Tier)
  # ============================================
  
  # Arcee Trinity Large - 400B MoE, good for agentic/creative tasks
  - model_name: trinity-free
    litellm_params:
      model: openrouter/arcee-ai/trinity-large-preview:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Qwen3 Coder - Best for code generation
  - model_name: qwen3-coder-free
    litellm_params:
      model: openrouter/qwen/qwen3-coder-480b-a35b:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 262000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # DeepSeek R1 - Reasoning model
  - model_name: deepseek-free
    litellm_params:
      model: openrouter/deepseek/deepseek-r1-0528:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 164000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Qwen3 Next - Good for RAG and tool use
  - model_name: qwen3-next-free
    litellm_params:
      model: openrouter/qwen/qwen3-next-80b-a3b-instruct:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 262000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # GLM 4.5 Air - Agent-focused with thinking mode
  - model_name: glm-free
    litellm_params:
      model: openrouter/z-ai/glm-4.5-air:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0
      output_cost_per_token: 0

  # NVIDIA Nemotron - Large context
  - model_name: nemotron-free
    litellm_params:
      model: openrouter/nvidia/nemotron-3-nano-30b-a3b:free
      api_key: os.environ/OPEN_ROUTER_KEY
    model_info:
      max_tokens: 256000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # ============================================
  # MOONSHOT/KIMI (Tertiary - Paid but Cheap)
  # ============================================
  
  # Kimi K2.5 - Best Chinese model, 256K context
  - model_name: kimi-k2.5
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # Kimi alias
  - model_name: kimi
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # Kimi local alias (for OpenClaw fallback)
  - model_name: kimi-local
    litellm_params:
      model: moonshot/kimi-k2.5
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000
      input_cost_per_token: 0
      output_cost_per_token: 0

  # Kimi Thinking (reasoning mode)
  - model_name: kimi-thinking
    litellm_params:
      model: moonshot/kimi-k2-thinking
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

  # Kimi Turbo (fast preview)
  - model_name: kimi-turbo
    litellm_params:
      model: moonshot/kimi-k2-turbo-preview
      api_key: os.environ/MOONSHOT_KIMI_KEY
      api_base: https://api.moonshot.ai/v1
    model_info:
      max_tokens: 256000

# ============================================
# ROUTER SETTINGS
# ============================================
router_settings:
  # Simple shuffle for load balancing
  routing_strategy: simple-shuffle
  # Retry failed requests
  num_retries: 3
  # Request timeout (10 minutes for long generations)
  timeout: 600
  # Allowed failures before cooldown
  allowed_fails: 3
  # Cooldown time after failures
  cooldown_time: 60
  # Fallback models when primary fails
  fallbacks: [
    # If local fails, try free cloud
    {"local": ["trinity-free", "deepseek-free", "kimi"]},
    {"qwen3-mlx": ["trinity-free", "deepseek-free", "kimi"]},
    # If free cloud fails, try paid
    {"trinity-free": ["kimi", "deepseek-free"]},
    {"deepseek-free": ["trinity-free", "kimi"]}
  ]

# ============================================
# GENERAL SETTINGS
# ============================================
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/LITELLM_DATABASE_URL
  # Flush spend logs to DB when queue reaches this size
  spend_log_queue_batch_size: 900

# ============================================
# LITELLM SETTINGS
# ============================================
litellm_settings:
  # Drop unsupported parameters instead of erroring
  drop_params: true
  # Disable streaming for simpler debugging
  disable_streaming: true
  # Verbose logging (set true for debugging)
  set_verbose: false
