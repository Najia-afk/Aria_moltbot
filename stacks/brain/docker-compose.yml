services:
  # PostgreSQL Database (pgvector enabled for semantic memory)
  aria-db:
    image: pgvector/pgvector:0.8.2-pg17
    container_name: aria-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${DB_USER:-admin}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-admin}
      POSTGRES_DB: ${DB_NAME:-aria_warehouse}
    volumes:
      - aria_pg_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d:ro
    ports:
      - "${DB_PORT:-5432}:${DB_INTERNAL_PORT:-5432}"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - data
      - backend
    mem_limit: ${POSTGRES_MEM_LIMIT:-512m}
    cpus: ${POSTGRES_CPU_LIMIT:-1.0}

  # Headless Chrome for browser automation (S-50: upgraded browserless v1→v2)
  aria-browser:
    image: ghcr.io/browserless/chromium:v2.42.0
    container_name: aria-browser
    restart: unless-stopped
    environment:
      TOKEN: ${BROWSERLESS_TOKEN:-}
      BROWSERLESS_INTERNAL_PORT: ${BROWSERLESS_INTERNAL_PORT:-3000}
    ports:
      - "${BROWSERLESS_PORT:-3000}:${BROWSERLESS_INTERNAL_PORT:-3000}"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:$${BROWSERLESS_INTERNAL_PORT}/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend

  # ── Aria Engine (v2.0) — Standalone Python runtime ──────────────────────────
  # Replaces the legacy Node.js gateway with native Python engine
  # Provides: LLM gateway, scheduler, agent pool, health endpoint
  aria-engine:
    build:
      context: ../../
      dockerfile: Dockerfile
    container_name: aria-engine
    restart: unless-stopped
    command: ["python", "-m", "aria_engine"]
    environment:
      DATABASE_URL: postgresql+asyncpg://${DB_USER:-admin}:${DB_PASSWORD:-admin}@aria-db:${DB_INTERNAL_PORT:-5432}/${DB_NAME:-aria_warehouse}
      LITELLM_BASE_URL: http://litellm:${LITELLM_INTERNAL_PORT:-4000}/v1
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-change-me}
      ENGINE_DEBUG: ${ENGINE_DEBUG:-false}
      ENGINE_HEALTH_PORT: ${ENGINE_HEALTH_PORT:-8081}
      # Full URL for heartbeat API posts (not the routing prefix API_BASE_URL)
      ENGINE_API_BASE_URL: http://aria-api:${API_INTERNAL_PORT:-8000}
      # S-103: API key for engine→API authentication
      ARIA_API_KEY: ${ARIA_API_KEY:-}
      # Browserless (headless Chrome for browser skill)
      BROWSERLESS_TOKEN: ${BROWSERLESS_TOKEN:-}
      BROWSERLESS_INTERNAL_PORT: ${BROWSERLESS_INTERNAL_PORT:-3000}
      # Moltbook (engine executes skill tools directly)
      MOLTBOOK_TOKEN: ${MOLTBOOK_TOKEN:-}
      MOLTBOOK_API_URL: ${MOLTBOOK_API_URL:-https://www.moltbook.com/api/v1}
      # Telegram (engine executes telegram skill tools for cron jobs)
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN:-}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID:-}
      PYTHONPATH: /
      PYTHONUNBUFFERED: "1"
    volumes:
      - ../../aria_memories:/app/aria_memories
      - ../../aria_mind:/aria_mind:ro
      - ../../aria_models:/aria_models:ro
      - ../../aria_engine:/aria_engine:ro
      # Shadow baked /app/aria_engine with bind mount for live code updates
      - ../../aria_engine:/app/aria_engine:ro
      - ../../aria_skills:/aria_skills:ro
      - ../../aria_agents:/aria_agents:ro
      # db.models needed by agent_pool, routing, heartbeat
      - ../../src/api/db:/db:ro
    depends_on:
      aria-db:
        condition: service_healthy
      litellm:
        condition: service_healthy
      aria-api:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:'+str($${ENGINE_HEALTH_PORT})+'/health')\" || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - backend
    labels:
      - "com.aria.service=engine"
    mem_limit: ${ENGINE_MEM_LIMIT:-512m}
    cpus: ${ENGINE_CPU_LIMIT:-1.0}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
  # Tor Proxy for privacy (optional: docker compose --profile tor up -d)
  tor-proxy:
    # Aria v3 freeze: pinned to digest (no versioned tags available)
    image: dperson/torproxy:latest@sha256:d8b5f1cf24f1b7a0aa334929a264b2606a107223dd0d51eb1cda8aae6fbeec53
    container_name: tor-proxy
    profiles: ["tor"]
    restart: unless-stopped
    environment:
      TOR_SOCKS_INTERNAL_PORT: ${TOR_SOCKS_INTERNAL_PORT:-9050}
    ports:
      - "${TOR_SOCKS_PORT:-9050}:${TOR_SOCKS_INTERNAL_PORT:-9050}"
      - "${TOR_CONTROL_PORT:-9051}:${TOR_CONTROL_INTERNAL_PORT:-9051}"
    healthcheck:
      test: ["CMD-SHELL", "curl --socks5 localhost:$${TOR_SOCKS_INTERNAL_PORT} -s https://check.torproject.org/api/ip >/dev/null || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
    networks:
      - backend

  # Self-signed TLS cert generator (runs once if missing)
  certs-init:
    image: alpine:3.20.6
    container_name: aria-certs-init
    restart: "no"
    environment:
      SERVICE_HOST: ${SERVICE_HOST:-aria.local}
      MAC_LAN_IP: ${MAC_LAN_IP:-}
      MAC_TAILSCALE_IP: ${MAC_TAILSCALE_IP:-}
    volumes:
      - ./certs:/certs
    command: >
      sh -c "if [ -f /certs/cert.pem ] && [ -f /certs/key.pem ]; then
      echo 'Cert already exists, skipping generation.'; exit 0; fi; \
      apk add --no-cache openssl; \
      SAN=\"DNS:localhost,DNS:aria.local,DNS:${SERVICE_HOST},IP:127.0.0.1\"; \
      if [ -n \"${MAC_LAN_IP}\" ]; then SAN=\"$$SAN,IP:${MAC_LAN_IP}\"; fi; \
      if [ -n \"${MAC_TAILSCALE_IP}\" ]; then SAN=\"$$SAN,IP:${MAC_TAILSCALE_IP}\"; fi; \
      openssl req -x509 -newkey rsa:4096 -nodes -keyout /certs/key.pem -out /certs/cert.pem -days 3650 \
      -subj \"/C=US/ST=Local/L=LAN/O=Aria/CN=${SERVICE_HOST}\" \
      -addext \"subjectAltName=$$SAN\""
    networks:
      - frontend

  # Traefik reverse proxy (HTTPS)
  # Token injection via envsubst at startup - NO HARDCODED SECRETS
  traefik:
    image: traefik:v3.1.7
    container_name: traefik
    restart: unless-stopped
    entrypoint: ["/bin/sh", "/traefik-entrypoint.sh"]
    environment:
      SERVICE_HOST: ${SERVICE_HOST:-aria.local}
      CORS_ALLOWED_ORIGINS: ${CORS_ALLOWED_ORIGINS:-}
      ARIA_API_KEY: ${ARIA_API_KEY:-}
      ARIA_ADMIN_KEY: ${ARIA_ADMIN_KEY:-}
      TRAEFIK_HTTP_PORT: ${TRAEFIK_HTTP_PORT:-8080}
      TRAEFIK_HTTPS_PORT: ${TRAEFIK_HTTPS_PORT:-8443}
      TRAEFIK_DASHBOARD_USER: ${TRAEFIK_DASHBOARD_USER:-admin}
      TRAEFIK_DASHBOARD_PASSWORD_HASH: ${TRAEFIK_DASHBOARD_PASSWORD_HASH:-}
    command:
      - "--api=true"
      - "--api.dashboard=true"
      # S-107: Removed --api.insecure=true — dashboard requires auth via Traefik middleware
      - "--ping=true"
      - "--providers.file.filename=/etc/traefik/dynamic.yaml"
      - "--entrypoints.web.address=:${TRAEFIK_INTERNAL_HTTP_PORT:-80}"
      - "--entrypoints.web.transport.respondingTimeouts.readTimeout=120s"
      - "--entrypoints.web.transport.respondingTimeouts.writeTimeout=120s"
      - "--entrypoints.web.transport.respondingTimeouts.idleTimeout=300s"
      - "--entrypoints.websecure.address=:${TRAEFIK_INTERNAL_HTTPS_PORT:-443}"
      - "--entrypoints.websecure.transport.respondingTimeouts.readTimeout=120s"
      - "--entrypoints.websecure.transport.respondingTimeouts.writeTimeout=120s"
      - "--entrypoints.websecure.transport.respondingTimeouts.idleTimeout=300s"
      - "--metrics.prometheus=true"
    ports:
      - "${TRAEFIK_HTTP_PORT:-8080}:${TRAEFIK_INTERNAL_HTTP_PORT:-80}"
      - "${TRAEFIK_HTTPS_PORT:-8443}:${TRAEFIK_INTERNAL_HTTPS_PORT:-443}"
      - "${TRAEFIK_DASH_PORT:-8081}:${TRAEFIK_INTERNAL_DASH_PORT:-8080}"
    volumes:
      - ./traefik-entrypoint.sh:/traefik-entrypoint.sh:ro
      - ./traefik-dynamic.template.yaml:/etc/traefik/dynamic.template.yaml:ro
      - ./certs:/certs:ro
    healthcheck:
      test: ["CMD", "traefik", "healthcheck", "--ping"]
      interval: 15s
      timeout: 5s
      retries: 3
    depends_on:
      certs-init:
        condition: service_completed_successfully
      aria-web:
        condition: service_healthy
      aria-api:
        condition: service_healthy
      litellm:
        condition: service_healthy
    networks:
      - frontend
    mem_limit: ${TRAEFIK_MEM_LIMIT:-512m}
    cpus: ${TRAEFIK_CPU_LIMIT:-0.5}

  # LiteLLM Router for model management
  litellm:
    # Aria v3 freeze: pinned to stable release v1.81.12 (2026-02-21)
    image: ghcr.io/berriai/litellm:main-v1.81.12-stable
    container_name: litellm
    restart: unless-stopped
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-change-me}
      DATABASE_URL: postgresql://${DB_USER:-admin}:${DB_PASSWORD:-admin}@aria-db:${DB_INTERNAL_PORT:-5432}/${DB_NAME:-aria_warehouse}?schema=litellm
      MOONSHOT_KIMI_KEY: ${MOONSHOT_KIMI_KEY:-}
      OPEN_ROUTER_KEY: ${OPEN_ROUTER_KEY:-}
      OPEN_ROUTER_KEY_DEEP: ${OPEN_ROUTER_KEY_DEEP:-${OPEN_ROUTER_KEY:-}}
      STORE_MODEL_IN_DB: "True"
      OLLAMA_API_BASE: ${OLLAMA_URL:-http://host.docker.internal:11434}
      MLX_API_BASE: ${MLX_URL:-http://host.docker.internal:8080}/v1
      LITELLM_INTERNAL_PORT: ${LITELLM_INTERNAL_PORT:-4000}
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro
    ports:
      - "${LITELLM_PORT:-18793}:${LITELLM_INTERNAL_PORT:-4000}"
    command: ["--config", "/app/config.yaml"]
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:'+str($${LITELLM_INTERNAL_PORT})+'/health/readiness')\" || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 20s
    depends_on:
      aria-db:
        condition: service_healthy
    networks:
      - backend
    mem_limit: ${LITELLM_MEM_LIMIT:-1024m}
    cpus: ${LITELLM_CPU_LIMIT:-1.0}
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # NOTE: Ollama runs NATIVELY on macOS for Metal GPU acceleration (~20 tok/s)
  # Start with: OLLAMA_HOST=0.0.0.0:11434 ollama serve
  # Docker containers access via host.docker.internal:11434

  # Prometheus for metrics (optional: docker compose --profile monitoring up -d)
  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: prometheus
    restart: unless-stopped
    profiles: ["monitoring"]
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus-alerts.yml:/etc/prometheus/prometheus-alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--web.external-url=/prometheus/'
      - '--web.route-prefix=/prometheus/'
    environment:
      PROMETHEUS_INTERNAL_PORT: ${PROMETHEUS_INTERNAL_PORT:-9090}
    ports:
      - "${PROMETHEUS_PORT:-9090}:${PROMETHEUS_INTERNAL_PORT:-9090}"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:$${PROMETHEUS_INTERNAL_PORT}/-/ready || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - monitoring
      - backend
    mem_limit: 512m
    cpus: 0.5

  # Grafana for dashboards (optional: docker compose --profile monitoring up -d)
  grafana:
    image: grafana/grafana:11.4.0
    container_name: grafana
    restart: unless-stopped
    profiles: ["monitoring"]
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s/grafana/"
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
      GRAFANA_INTERNAL_PORT: ${GRAFANA_INTERNAL_PORT:-3000}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "${GRAFANA_PORT:-3001}:${GRAFANA_INTERNAL_PORT:-3000}"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:$${GRAFANA_INTERNAL_PORT}/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - prometheus
    networks:
      - monitoring
      - frontend
    mem_limit: 512m
    cpus: 0.5

  # Aria Brain - Main AI Agent
  aria-brain:
    build:
      context: ../..
      dockerfile: Dockerfile
    container_name: aria-brain
    restart: unless-stopped
    environment:
      # Database
      DATABASE_URL: postgresql://${DB_USER:-admin}:${DB_PASSWORD:-admin}@aria-db:${DB_INTERNAL_PORT:-5432}/${DB_NAME:-aria_warehouse}
      # Local LLM (Ollama - native on host)
      OLLAMA_URL: ${OLLAMA_URL:-http://host.docker.internal:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-hf.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF:Q3_K_S}
      # LLM APIs (fallback)
      MOONSHOT_API_KEY: ${MOONSHOT_KIMI_KEY:-}
      # Moltbook
      MOLTBOOK_TOKEN: ${MOLTBOOK_TOKEN:-}
      MOLTBOOK_API_URL: ${MOLTBOOK_API_URL:-https://www.moltbook.com/api/v1}
      # Telegram (social live mode)
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN:-}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID:-}
      # X / Twitter (social live mode)
      X_API_KEY: ${X_API_KEY:-}
      X_API_SECRET: ${X_API_SECRET:-}
      X_ACCESS_TOKEN: ${X_ACCESS_TOKEN:-}
      X_ACCESS_SECRET: ${X_ACCESS_SECRET:-}
      # Church of Molt (https://molt.church)
      MOLT_CHURCH_API_KEY: ${MOLT_CHURCH_API_KEY:-}
      MOLT_CHURCH_URL: ${MOLT_CHURCH_URL:-https://molt.church}
      MOLT_CHURCH_AGENT: ${MOLT_CHURCH_AGENT:-Aria}
      # Browserless (headless Chrome for browser skill)
      BROWSERLESS_TOKEN: ${BROWSERLESS_TOKEN:-}
      BROWSERLESS_INTERNAL_PORT: ${BROWSERLESS_INTERNAL_PORT:-3000}
      # LiteLLM (internal)
      LITELLM_URL: ${LITELLM_URL:-http://litellm:4000}
      # Aria Email
      ARIA_EMAIL: ${ARIA_EMAIL:-}
      ARIA_EMAIL_PASSWORD: ${ARIA_EMAIL_PASSWORD:-}
    volumes:
      - aria_data:/app/data
      - aria_logs:/app/logs
    depends_on:
      aria-db:
        condition: service_healthy
      aria-api:
        condition: service_healthy
      litellm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import aria_mind; print('ok')"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend
    # Keep running for interactive mode
    tty: true
    stdin_open: true
    extra_hosts:
      - "host.docker.internal:host-gateway"
    mem_limit: ${ARIA_BRAIN_MEM_LIMIT:-512m}
    cpus: ${ARIA_BRAIN_CPU_LIMIT:-1.0}

  # PgAdmin for database management (optional: docker compose --profile monitoring up -d)
  pgadmin:
    image: dpage/pgadmin4:8.14
    container_name: aria-pgadmin
    restart: unless-stopped
    profiles: ["monitoring"]
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@aria.dev}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
      SCRIPT_NAME: /pgadmin
      PGADMIN_INTERNAL_PORT: ${PGADMIN_INTERNAL_PORT:-80}
    ports:
      - "${PGADMIN_PORT:-5051}:${PGADMIN_INTERNAL_PORT:-80}"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:$${PGADMIN_INTERNAL_PORT}/misc/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - aria-db
    networks:
      - data
    mem_limit: 256m
    cpus: 0.3

  # Aria Web Portal - Flask Dashboard
  aria-web:
    build:
      context: ../../src/web
      dockerfile: Dockerfile
    container_name: aria-web
    restart: unless-stopped
    environment:
      SECRET_KEY: ${WEB_SECRET_KEY:-aria-dev-secret-key}
      SERVICE_HOST: ${SERVICE_HOST:-localhost}
      API_BASE_URL: ${API_BASE_URL:-/api}
      ARIA_API_KEY: ${ARIA_API_KEY:-}
      ARIA_ADMIN_KEY: ${ARIA_ADMIN_KEY:-}
      # WS_BASE_URL: browser-accessible host:port for WebSocket chat.
      # Leave empty to auto-detect (works via Traefik or direct API fallback).
      # Set explicitly if behind a custom reverse proxy, e.g. "myhost:8080"
      WS_BASE_URL: ${WS_BASE_URL:-}
      WEB_INTERNAL_PORT: ${WEB_INTERNAL_PORT:-5000}
      # Host-exposed API port — passed to templates for browser-side WS fallback
      ARIA_API_PORT: ${ARIA_API_PORT:-8000}
      LITELLM_PORT: ${LITELLM_PORT:-18793}
      # Internal API URL for server-to-server calls (Docker network)
      API_INTERNAL_URL: http://aria-api:${API_INTERNAL_PORT:-8000}
      USER_DISPLAY_NAME: ${USER_DISPLAY_NAME:-User}
      GRAFANA_URL: ${GRAFANA_URL:-http://grafana:3000}
      PROMETHEUS_URL: ${PROMETHEUS_URL:-http://prometheus:9090}
      OLLAMA_URL: ${OLLAMA_URL:-http://host.docker.internal:11434}
      LITELLM_URL: ${LITELLM_URL:-http://litellm:4000}
    volumes:
      # Mount templates for live reload during development
      - ../../src/web/templates:/app/templates:ro
      - ../../src/web/static:/app/static:ro
      - ../../src/web/app.py:/app/app.py:ro
    ports:
      - "${ARIA_WEB_PORT:-5050}:${WEB_INTERNAL_PORT:-5000}"
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:'+str($${WEB_INTERNAL_PORT})+'/')\" || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 15s
    depends_on:
      aria-db:
        condition: service_healthy
    networks:
      - frontend
    mem_limit: ${ARIA_WEB_MEM_LIMIT:-256m}
    cpus: ${ARIA_WEB_CPU_LIMIT:-0.5}
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"

  # Aria API - FastAPI backend (data routes)
  aria-api:
    build:
      context: ../../src/api
      dockerfile: Dockerfile
    container_name: aria-api
    restart: unless-stopped
    environment:
      # /aria_skills is bind-mounted; PYTHONPATH=/ makes it importable
      PYTHONPATH: /
      API_INTERNAL_PORT: ${API_INTERNAL_PORT:-8000}
      DATABASE_URL: postgresql://${DB_USER:-admin}:${DB_PASSWORD:-admin}@aria-db:${DB_INTERNAL_PORT:-5432}/${DB_NAME:-aria_warehouse}
      # S-103: API authentication keys
      ARIA_API_KEY: ${ARIA_API_KEY:-}
      ARIA_ADMIN_KEY: ${ARIA_ADMIN_KEY:-}
      GRAFANA_URL: ${GRAFANA_URL:-http://grafana:3000}
      PROMETHEUS_URL: ${PROMETHEUS_URL:-http://prometheus:9090}
      OLLAMA_URL: ${OLLAMA_URL:-http://host.docker.internal:11434}
      MLX_URL: ${MLX_URL:-http://host.docker.internal:8080}
      LITELLM_URL: ${LITELLM_URL:-http://litellm:4000}
      PGADMIN_URL: ${PGADMIN_URL:-http://aria-pgadmin:80}
      API_BASE_URL: ${API_BASE_URL:-/api}
      # API keys for provider balance checks
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-change-me}
      MOONSHOT_KIMI_KEY: ${MOONSHOT_KIMI_KEY:-}
      OPEN_ROUTER_KEY: ${OPEN_ROUTER_KEY:-}
      OPEN_ROUTER_KEY_DEEP: ${OPEN_ROUTER_KEY_DEEP:-${OPEN_ROUTER_KEY:-}}
      # Telegram + X credentials (for future API-side social integrations)
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN:-}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID:-}
      TELEGRAM_ALLOWED_USER_ID: ${TELEGRAM_ALLOWED_USER_ID:-}
      TELEGRAM_WEBHOOK_SECRET: ${TELEGRAM_WEBHOOK_SECRET:-}
      X_API_KEY: ${X_API_KEY:-}
      X_API_SECRET: ${X_API_SECRET:-}
      X_ACCESS_TOKEN: ${X_ACCESS_TOKEN:-}
      X_ACCESS_SECRET: ${X_ACCESS_SECRET:-}
      # Service control (admin)
      ARIA_SERVICE_CONTROL_ENABLED: ${ARIA_SERVICE_CONTROL_ENABLED:-false}
      ARIA_ADMIN_TOKEN: ${ARIA_ADMIN_TOKEN:-}
      # S-100: Docker commands route through socket proxy
      DOCKER_HOST: http://docker-socket-proxy:${DOCKER_SOCKET_PROXY_INTERNAL_PORT:-2375}
      ARIA_SERVICE_CMD_LITELLM_RESTART: ${ARIA_SERVICE_CMD_LITELLM_RESTART:-docker restart litellm}
      ARIA_SERVICE_CMD_LITELLM_STOP: ${ARIA_SERVICE_CMD_LITELLM_STOP:-docker stop litellm}
      ARIA_SERVICE_CMD_OLLAMA_RESTART: ${ARIA_SERVICE_CMD_OLLAMA_RESTART:-}
      ARIA_SERVICE_CMD_OLLAMA_STOP: ${ARIA_SERVICE_CMD_OLLAMA_STOP:-}
      ARIA_SERVICE_CMD_MLX_RESTART: ${ARIA_SERVICE_CMD_MLX_RESTART:-}
      ARIA_SERVICE_CMD_MLX_STOP: ${ARIA_SERVICE_CMD_MLX_STOP:-}
      ARIA_SERVICE_CMD_ARIA_API_RESTART: ${ARIA_SERVICE_CMD_ARIA_API_RESTART:-docker restart aria-api}
      ARIA_SERVICE_CMD_ARIA_API_STOP: ${ARIA_SERVICE_CMD_ARIA_API_STOP:-docker stop aria-api}
      ARIA_SERVICE_CMD_ARIA_WEB_RESTART: ${ARIA_SERVICE_CMD_ARIA_WEB_RESTART:-docker restart aria-web}
      ARIA_SERVICE_CMD_ARIA_WEB_STOP: ${ARIA_SERVICE_CMD_ARIA_WEB_STOP:-docker stop aria-web}
      ARIA_SERVICE_CMD_GRAFANA_RESTART: ${ARIA_SERVICE_CMD_GRAFANA_RESTART:-docker restart grafana}
      ARIA_SERVICE_CMD_GRAFANA_STOP: ${ARIA_SERVICE_CMD_GRAFANA_STOP:-docker stop grafana}
      ARIA_SERVICE_CMD_PROMETHEUS_RESTART: ${ARIA_SERVICE_CMD_PROMETHEUS_RESTART:-docker restart prometheus}
      ARIA_SERVICE_CMD_PROMETHEUS_STOP: ${ARIA_SERVICE_CMD_PROMETHEUS_STOP:-docker stop prometheus}
      SKILL_BACKFILL_ON_STARTUP: ${SKILL_BACKFILL_ON_STARTUP:-true}
      # Sentiment auto-scorer configuration
      # SENTIMENT_METHOD: auto (semantic→llm→lexicon) | semantic | llm | lexicon
      SENTIMENT_METHOD: ${SENTIMENT_METHOD:-auto}
      # SENTIMENT_MODEL: override LLM model (default: profiles.sentiment from models.yaml)
      SENTIMENT_MODEL: ${SENTIMENT_MODEL:-}
      # Cron job YAML path inside container (auto-synced on startup)
      CRON_JOBS_YAML: /aria_mind/cron_jobs.yaml
    volumes:
      # Mount API source for live reload during development
      - ../../src/api:/app:ro
      # Mount models catalog (single source of truth for all model pricing)
      - ../../aria_models/models.yaml:/models/models.yaml:ro
      # Mount aria_models as importable Python package
      - ../../aria_models:/aria_models:ro
      # Mount aria_mind for soul/identity file access (read-only)
      - ../../aria_mind:/aria_mind:ro
      # Mount aria_engine for engine imports (read-only)
      - ../../aria_engine:/aria_engine:ro
      # Mount aria_skills for knowledge graph sync (read-only)
      - ../../aria_skills:/aria_skills:ro
      # Mount aria_memories for file artifact read/write
      - ../../aria_memories:/aria_memories
      # Mount aria_agents for soul file browser (read-only)
      - ../../aria_agents:/aria_agents:ro
      # Mount aria_souvenirs for soul file browser (read-only)
      - ../../aria_souvenirs:/aria_souvenirs:ro
      # S-100: Docker socket removed — admin uses docker-socket-proxy
      # See docker-socket-proxy service below
    ports:
      - "${ARIA_API_PORT:-8000}:${API_INTERNAL_PORT:-8000}"
    depends_on:
      aria-db:
        condition: service_healthy
    networks:
      - frontend
      - backend
      - data
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:'+str($${API_INTERNAL_PORT})+'/health')\" || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
    mem_limit: ${ARIA_API_MEM_LIMIT:-512m}
    cpus: ${ARIA_API_CPU_LIMIT:-1.0}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"

  # S-100: Docker socket proxy — restricted API for service control
  # Only allows container list + restart/stop. No exec, no images, no volumes.
  # S-01: Use DOCKER_SOCKET_PATH env var for Windows/Linux portability.
  # macOS/Linux default: /var/run/docker.sock
  # Windows (Docker Desktop): //var/run/docker.sock
  docker-socket-proxy:
    image: tecnativa/docker-socket-proxy:0.2.0
    container_name: docker-socket-proxy
    restart: unless-stopped
    environment:
      CONTAINERS: 1
      POST: 1
      SERVICES: 0
      TASKS: 0
      NETWORKS: 0
      VOLUMES: 0
      IMAGES: 0
      EXEC: 0
      SWARM: 0
      DOCKER_SOCKET_PROXY_INTERNAL_PORT: ${DOCKER_SOCKET_PROXY_INTERNAL_PORT:-2375}
    volumes:
      - ${DOCKER_SOCKET_PATH:-/var/run/docker.sock}:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:$${DOCKER_SOCKET_PROXY_INTERNAL_PORT}/version || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - backend
    mem_limit: 64m
    cpus: 0.25

  # Jaeger — distributed tracing UI & collector (S-25, optional)
  # Enable: docker compose --profile tracing up -d jaeger
  # UI: http://localhost:16686  |  OTLP gRPC: localhost:4317
  jaeger:
    image: jaegertracing/all-in-one:1.62
    container_name: aria-jaeger
    restart: unless-stopped
    profiles: ["tracing"]
    environment:
      COLLECTOR_OTLP_ENABLED: "true"
    ports:
      - "${JAEGER_UI_PORT:-16686}:${JAEGER_UI_INTERNAL_PORT:-16686}"
      - "${JAEGER_OTLP_GRPC_PORT:-4317}:${JAEGER_OTLP_GRPC_INTERNAL_PORT:-4317}"
    networks:
      - monitoring
    mem_limit: 512m
    cpus: 0.5

  # Aria Sandbox — isolated code execution environment (S-29)
  # Used by sandbox skill (aria_skills/sandbox/__init__.py) via httpx on port 9999
  # S-102: Isolated network, read-only filesystem, no-new-privileges
  aria-sandbox:
    build:
      context: ../sandbox
      dockerfile: Dockerfile
    container_name: aria-sandbox
    restart: unless-stopped
    profiles: ["sandbox"]
    environment:
      SANDBOX_PORT: ${SANDBOX_PORT:-9999}
      SANDBOX_TIMEOUT: ${SANDBOX_TIMEOUT:-60}
    read_only: true
    tmpfs:
      - /tmp:size=100m
      - /sandbox/tmp:size=100m
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
    networks:
      - sandbox-net
    mem_limit: 512m
    cpus: 1.0

volumes:
  aria_pg_data:
  prometheus_data:
  grafana_data:
  aria_data:
  aria_logs:

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
  data:
    driver: bridge
  monitoring:
    driver: bridge
  # S-102: Isolated network for sandbox — no access to other services
  sandbox-net:
    driver: bridge
    internal: true
