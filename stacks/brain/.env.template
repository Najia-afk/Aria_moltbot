# Aria Brain Stack Environment Configuration
# Copy this to .env and customize for your deployment
# NEVER commit .env files with real credentials to version control!
#
# REQUIRED: Set SERVICE_HOST to your server's IP or domain name
# Example: SERVICE_HOST=192.168.1.53 or SERVICE_HOST=aria.myserver.com

# ============================================
# DEPLOYMENT TARGET (REQUIRED)
# ============================================
# Your server's accessible IP or hostname
# For LAN: use your machine's IP (e.g., 192.168.1.53)
# For Tailscale: use Tailscale IP (e.g., 100.71.168.4)
# For public: use domain name (e.g., aria.example.com)
SERVICE_HOST=your_server_ip_or_domain

# Protocol for external URLs (http or https)
SERVICE_PROTOCOL=https

# ============================================
# DATABASE (aria_warehouse)
# ============================================
DB_USER=aria_admin
DB_PASSWORD=change_this_secure_password
DB_NAME=aria_warehouse

# ============================================
# API ROUTING
# ============================================
API_BASE_URL=/api

# ============================================
# OPENCLAW / CLAWDBOT
# ============================================
# Generate token with: openssl rand -hex 24
CLAWDBOT_TOKEN=generate_with_openssl_rand_hex_24
# This URL is auto-generated from SERVICE_HOST - no need to edit
CLAWDBOT_PUBLIC_URL=${SERVICE_PROTOCOL}://${SERVICE_HOST}/clawdbot/?session=main&token=${CLAWDBOT_TOKEN}

# ============================================
# WEB
# ============================================
# Generate with: python3 -c "import secrets; print(secrets.token_hex(32))"
WEB_SECRET_KEY=generate_random_secret_key

# ============================================
# API KEYS (get from providers)
# ============================================

# Moonshot/Kimi API Key (China-based, fast, cheap)
# Get from: https://platform.moonshot.cn/
MOONSHOT_KIMI_KEY=

# OpenRouter API Key (aggregated free models)
# Get from: https://openrouter.ai/
# Leave empty to use free tier only
OPEN_ROUTER_KEY=

# ============================================
# LITELLM (Internal Router)
# ============================================
# Internal key for LiteLLM authentication
# Generate with: openssl rand -hex 16
LITELLM_MASTER_KEY=sk-aria-local-key

# ============================================
# MOLTBOOK - Social Platform for AI Agents
# ============================================
# Register at: https://moltbook.com/skill.md
MOLTBOOK_API_URL=https://www.moltbook.com/api/v1
MOLTBOOK_API_KEY=
# Leave MOLTBOOK_TOKEN empty to disable auto-posting on startup
MOLTBOOK_TOKEN=

# ============================================
# MONITORING
# ============================================
GRAFANA_PASSWORD=change_this_password
PGADMIN_EMAIL=admin@your-domain.com
PGADMIN_PASSWORD=change_this_password

# ============================================
# BROWSERLESS (Optional)
# ============================================
BROWSERLESS_TOKEN=

# ============================================
# SOCIAL PLATFORMS (Optional)
# ============================================

# Telegram Bot Token
# Get from: @BotFather on Telegram
TELEGRAM_BOT_TOKEN=
TELEGRAM_CHAT_ID=

# ============================================
# TOR (Optional)
# ============================================
TOR_PROXY=socks5://tor-proxy:9050

# ============================================
# LOCAL LLM Configuration
# ============================================
# Per SOUL.md: local models preferred over cloud

# Docker host networking:
# - macOS/Windows Docker Desktop: host.docker.internal
# - Linux with --add-host: host.docker.internal
# - Linux host network mode: localhost
DOCKER_HOST_IP=host.docker.internal

# Ollama URL (native service recommended for GPU acceleration)
# Default assumes Ollama runs natively on host at port 11434
OLLAMA_URL=http://${DOCKER_HOST_IP}:11434
OLLAMA_MODEL=hf.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF:Q3_K_S

# MLX URL (macOS only - native Metal acceleration)
# Set to empty to disable MLX
MLX_URL=http://${DOCKER_HOST_IP}:8080
# Set MLX_ENABLED=false if not using MLX or insufficient RAM
MLX_ENABLED=true

# ============================================
# ARIA EMAIL (Optional)
# ============================================
ARIA_EMAIL=
ARIA_EMAIL_PASSWORD=

# ============================================
# ARIA X.com / Twitter (Optional)
# ============================================
ARIA_LOGIN=
ARIA_X_PASSWORD=
ARIA_X_handle=
ct0_token=
auth_token=

# ============================================
# RESOURCE LIMITS
# ============================================
# Adjust based on your server's resources
# Minimum recommended: 8GB RAM total

POSTGRES_MEM_LIMIT=512m
POSTGRES_CPU_LIMIT=1.0

ARIA_BRAIN_MEM_LIMIT=512m
ARIA_BRAIN_CPU_LIMIT=1.0

# Increase if running local LLM in container (not recommended)
OLLAMA_MEM_LIMIT=5g
OLLAMA_CPU_LIMIT=3.0

ARIA_WEB_MEM_LIMIT=256m
ARIA_WEB_CPU_LIMIT=0.5

ARIA_API_MEM_LIMIT=256m
ARIA_API_CPU_LIMIT=0.5

CLAWDBOT_MEM_LIMIT=1g
CLAWDBOT_CPU_LIMIT=1.0

LITELLM_MEM_LIMIT=1024m
LITELLM_CPU_LIMIT=1.0
